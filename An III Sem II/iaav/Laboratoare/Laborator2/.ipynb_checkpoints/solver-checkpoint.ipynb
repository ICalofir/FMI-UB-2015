{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KKKTX2NeYPjc"
   },
   "source": [
    "# Implementarea unei Retele Neurale cu un singur strat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqRC3npbZGHw"
   },
   "source": [
    "In acest exercitiu o sa creati o retea neurala cu un singur strat. Stratul ascuns este conectat la tot inputul si este folosit pentru a clasifica si a testa imagini din datasetul CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "UJniLsBbBZy9"
   },
   "source": [
    ">[Implementarea unei Retele Neurale cu un singur strat](#scrollTo=KKKTX2NeYPjc)\n",
    "\n",
    ">[Definirea retelei](#scrollTo=Q_ogHHgy1Wq8)\n",
    "\n",
    ">[Data de intrare de test](#scrollTo=gE1FLhMn_mvN)\n",
    "\n",
    ">[Propagare inainte (Forward pass)](#scrollTo=3yu3ho1zB7q3)\n",
    "\n",
    ">>[Calcularea scorurilor](#scrollTo=3yu3ho1zB7q3)\n",
    "\n",
    ">>[Calcularea erorii](#scrollTo=kYpWbCtx5yYL)\n",
    "\n",
    ">[Propagare inapoi (Backpropagation)](#scrollTo=kX87ZgHlrZgN)\n",
    "\n",
    ">[Antrenarea retelei (training)](#scrollTo=955D0vlQrZgX)\n",
    "\n",
    ">[BONUS](#scrollTo=VPt6-ZetZkyB)\n",
    "\n",
    ">[Incarcarea datasetului - CIFAR-10](#scrollTo=VPt6-ZetZkyB)\n",
    "\n",
    ">>>[Preprocesarea datasetului](#scrollTo=cTCHuWmCv4aZ)\n",
    "\n",
    ">>[Antrenarea retelei pe CIFAR-10](#scrollTo=6829NYnCw4Mc)\n",
    "\n",
    ">[Debug procesul de antrenare](#scrollTo=kqZWQgqJrZhK)\n",
    "\n",
    ">[Tunarea hiperparametrilor](#scrollTo=VOSpkebYrZhV)\n",
    "\n",
    ">[Rularea pe datele de test](#scrollTo=BqzwoAYorZhn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KtCFA_axXuSP"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import mnist\n",
    "# initializare pentru matplotlib\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # setarea dimensiunii plot-urilor \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# auto-reloading pentru module externe\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" intoarce eroarea relativa \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_ogHHgy1Wq8"
   },
   "source": [
    "# Definirea retelei\n",
    "\n",
    "Vom defini reteaua cu un singur layer ascuns cu o clasa. Reteaua primeste ca input x de dimensiunea \"input_dim\", are un strat ascuns de dimensiune \"hidden_dim\". Aceasta clasifica input-ul in una din cele \"num_classes\". \n",
    "La iesire, reteaua trece score-urile calculate pe ultimul strat fully_connected printr-o functie **softmax** pentru a scoate probabilitati. \n",
    "\n",
    "Functia obiectiv este **cross-entropia**. Adaugam ca regularizare **L2_norm** pe matricile parametrilor (fara bias). Reteaua are o functie non-liniara dupa primul strat fully_connected. Vom folosi **ReLU** pentru aceasta nonliniaritate.\n",
    "\n",
    "Reteaua are urmatoarea arhitectura: input - fully_connected - ReLU - fully_connected - softmax.\n",
    "Output-ul celui de-al doilea strat de fully_connected reprezinta score-urile claselor.\n",
    "\n",
    "Scheletul de cod pentru **ShallowNet** de afla in fisierul **shallow_net.py** pentru a putea lucra in paralel la cele doua fisiere. Alte functii ajutatoare pe care le veti intalni pe parcurs se afla in **utils.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dFZKcJ4Lvtag"
   },
   "outputs": [],
   "source": [
    "%%writefile shallow_net.py\n",
    "import numpy as np\n",
    "\n",
    "class ShallowNet():\n",
    "\n",
    "    class fully_connected:\n",
    "        \"\"\"\n",
    "        Unitatea de baza in reteaua noastra cu un singur strat va fi stratul \n",
    "        fully_connected. Am decis sa il implementam ca o clasa distincta pentru\n",
    "        a fi mai usor sa stocam si sa accesam parametrii stratului in timpul\n",
    "        propagarii inainte si inapoi.\n",
    "        Veti avea de implementat mai multe functii in aceasta clasa \n",
    "        \"\"\"\n",
    "        def __init__(self, w, b, activation=\"relu\"):\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "            self.act = activation\n",
    "\n",
    "        def linear_forward(self):\n",
    "            h = None\n",
    "            ####################################################################\n",
    "            # TODO: Calcularea unui forward pass printr-un layer de tip fully  #\n",
    "            # connected fara non-liniaritate ca o combinatie liniara a         #\n",
    "            # weight-urilor self.w cu inputul self.x si cu bias-ul self.b      #\n",
    "            # Intoarcerea rezultatului h.                                      #\n",
    "            ####################################################################\n",
    "            pass\n",
    "            ####################################################################\n",
    "            #                         END OF YOUR CODE                         #\n",
    "            ####################################################################\n",
    "            return h\n",
    "\n",
    "        def linear_backward(self, dout):\n",
    "            dx, dw, db = None, None, None\n",
    "            ####################################################################\n",
    "            # TODO: Calcularea unui backward passprintr-un layer de tip fully  #\n",
    "            # connected fara non-liniaritate. Avem urmatoarele iesiri:         #\n",
    "            # * dx - gradientul dout/dx; Hint: out = xw + b                   #\n",
    "            # * dw - gradientul dout/dw; Hint: out = xw + b                   #\n",
    "            # * db - gradientul dout/db; Hint: out = xw + b                   #\n",
    "            ####################################################################\n",
    "            pass\n",
    "            ####################################################################\n",
    "            #                         END OF YOUR CODE                         #\n",
    "            ####################################################################\n",
    "            return dx, dw, db    \n",
    "\n",
    "\n",
    "        def relu(self, h):\n",
    "            out = None\n",
    "            ####################################################################\n",
    "            # TODO: Aplicarea unei functiei ReLU peste rezultatul primului     #\n",
    "            # strat ce este stocat in h                                   #\n",
    "            ####################################################################\n",
    "            pass\n",
    "            ####################################################################\n",
    "            #                         END OF YOUR CODE                         #\n",
    "            ####################################################################\n",
    "            return out\n",
    "\n",
    "        def relu_backward(self, dout):\n",
    "            dx = None\n",
    "            ####################################################################\n",
    "            # TODO: Propagarea gradientului prin functia ReLU.                 #\n",
    "            # Hint: trebuie sa \"inmultim\" gradientul upstream dout cu          #\n",
    "            # gradientul efectiv al functiei ReLU si sa intoarcem rezultatul   #\n",
    "            #  \"inmultirii\" in dx (chain rule).\n",
    "            ####################################################################\n",
    "            pass\n",
    "            ####################################################################\n",
    "            #                         END OF YOUR CODE                         #\n",
    "            ####################################################################\n",
    "            return dx\n",
    "\n",
    "        def sigmoid(self):\n",
    "            out = np.ones_like(self.h) / (1 + np.exp(-self.h))\n",
    "            return out\n",
    "\n",
    "        def sigmoid_backward(self, dout):\n",
    "            out_sigmoid = np.ones_like(self.h) / (1 + np.exp(-self.h))\n",
    "            dx = dout * out_sigmoid * (1 - out_sigmoid)\n",
    "            return dx\n",
    "\n",
    "        def tanh(self):\n",
    "            out = (np.exp(self.h) - np.exp(-self.h)) / (np.exp(self.h) + np.exp(-self.h))\n",
    "            return out\n",
    "\n",
    "        def tanh_backward(self, dout):\n",
    "            out_tanh = (np.exp(self.h) - np.exp(-self.h)) / (np.exp(self.h) + np.exp(-self.h))\n",
    "            dx = dout * (1 - out_tanh ** 2)\n",
    "            return dx\n",
    "\n",
    "        def forward(self, x):\n",
    "            self.x = x\n",
    "            self.h = self.linear_forward()\n",
    "            if self.act == \"relu\":\n",
    "                self.h_act = self.relu(self.h)\n",
    "                return self.h_act\n",
    "            elif self.act == \"sigmoid\":\n",
    "                self.h_act = self.sigmoid()\n",
    "                return self.h_act\n",
    "            elif self.act == \"tanh\":\n",
    "                self.h_act = self.tanh()\n",
    "                return self.h_act\n",
    "            else:\n",
    "                return self.h\n",
    "\n",
    "        def backward(self, dout):\n",
    "            if self.act == \"relu\":\n",
    "                self.dh = self.relu_backward(dout)\n",
    "                dx, dw, db = self.linear_backward(self.dh)\n",
    "                return dx, dw, db\n",
    "            elif self.act == \"sigmoid\":\n",
    "                self.dh = self.sigmoid_backward(dout)\n",
    "                dx, dw, db = self.linear_backward(self.dh)\n",
    "                return dx, dw, db\n",
    "            elif self.act == \"tanh\":\n",
    "                self.dh = self.tanh_backward(dout)\n",
    "                dx, dw, db = self.linear_backward(self.dh)\n",
    "                return dx, dw, db\n",
    "            else:\n",
    "                dx, dw, db = self.linear_backward(dout)\n",
    "                return dx, dw, db    \n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "               input_dim=3*32*32,\n",
    "               hidden_dim=100,\n",
    "               num_classes=10,\n",
    "               std=1e-4,\n",
    "               activation_fn=\"relu\"):\n",
    "        \"\"\"\n",
    "        Initializarea modelului. Parametrii sunt initializati cu numere mici random\n",
    "        cu o distributie calibrata dupa numarul de neuroni de intrare. Bias-urile \n",
    "        sunt initializate cu zero-uri.\n",
    "        Parametrii modelului sunt stocati intr-un dictionar self.params.\n",
    "        Cheile dictionar-ului sunt:\n",
    "        fc1_w: weight-urile primului strat (input_dim, hidden_dim)\n",
    "        fc1_b: bias-urile primului strat (hidden_dim,)\n",
    "        fc2_w: weight-urile celui de-al doilea strat (hidden_dim, num_classes)\n",
    "        fc2_b: bias-urile celui de-al doilea strat (num_classes,)\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.params = {}\n",
    "\n",
    "        # Pentru a initializa weight-urile cu calibrarea variantei in functie de tipul functiei\n",
    "        # de activare, putem inlocui std cu urmatoarele :\n",
    "        # * std = np.sqrt(1.0/input_dim) (pentru functii de activare sigmoid, tanh etc.)\n",
    "        # * std = np.sqrt(2.0/input_dim) (pentru relu)\n",
    "        self.params['fc1_w'] = std * np.random.randn(input_dim, hidden_dim)\n",
    "        self.params['fc1_b'] = np.zeros(hidden_dim)\n",
    "        self.params['fc2_w'] = std * np.random.randn(hidden_dim, num_classes)\n",
    "        self.params['fc2_b'] = np.zeros(num_classes)\n",
    "\n",
    "        self.fc1 = self.fully_connected(w=self.params['fc1_w'], b=self.params['fc1_b'], activation=activation_fn)\n",
    "        self.fc2 = self.fully_connected(w=self.params['fc2_w'], b=self.params['fc2_b'], activation=None)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Calcularea functiei obiectiv si a gradientilor\n",
    "\n",
    "        Input:\n",
    "        - X: imagini de dimensiune (batch_size, input_dim). Fiecare X[i] este un exemplu.\n",
    "        - labels: Vector de etichete. labels[i] este label-ul pentru X[i], si fiecare labels[i] este un\n",
    "          intreg intre 0 <= labels[i] < num_classes. \n",
    "          Acest parametru este optional. Daca este omis atunci vom intoarce score-urile.\n",
    "          Daca este specificat facem o parcurgere inainte si inapoi (feed-forward si backpropagation)\n",
    "          si intoarcem rezultatul functiei de cost si gradientii calculati.\n",
    "        - reg: parametrul pentru regularizare.\n",
    "\n",
    "        Iesire:\n",
    "        Daca label-urile sunt omise, atunci intoarcem score-urile calculate de feed_forward pass \n",
    "        cu o matrice de dimensiunea (batch_size, num_classes), unde fiecare scor[i, c] reprezinta score-ul \n",
    "        clasei c pentru inputul X[i].\n",
    "\n",
    "        Daca label-urile nu sunt omise atunci intoarcem un tuplu:\n",
    "        * rezultat functie de cost (include si termenul de regularizare) pentru un batch\n",
    "        de exemple\n",
    "        * gradientii: un dictionar care mapeaza pentru fiecare parametru gradientul acelui parametru. \n",
    "        Are aceleasi chei ca dictionarul pentru parametrii self.params.\n",
    "        \"\"\"\n",
    "        self.l2_reg = reg\n",
    "        fc1_w, fc1_b = self.params['fc1_w'], self.params['fc1_b']\n",
    "        fc2_w, fc2_b = self.params['fc2_w'], self.params['fc2_b']\n",
    "        batch_size, input_dim = X.shape\n",
    "\n",
    "        # Calcularea scor-urilor cu o propagare inainte prin retea (forward pass)\n",
    "        scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Calcularea unui forward pass din care sa rezulte scorurile finale   #\n",
    "        # corespunzatoare fiecarei clase pentru fiecare input intr-o matrice de     #\n",
    "        # dimensiuni (batch_size, num_classes).                                     #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # Daca label-urile sunt omise:\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Altfel, calculam rezultatul functiei de cost\n",
    "        loss = None\n",
    "        #############################################################################\n",
    "        # TODO: Terminam calcularea unei propagari inainte prin retea (forward pass)#\n",
    "        # Calculam loss-ul (rezultatul functiei obiectiv pentru intrari. Acesta     #\n",
    "        # include  loss-ul datelor si regularizarea cu norma L2. Rezultatul trebuie #\n",
    "        # sa fie un scalar. Regularizarea trebuie aplicata pentru fc1_w si fc2_w.   #\n",
    "        # Pentru functia de cost folosim functia obiectiv de clasificare:           #\n",
    "        # cross-entropia.                                                           #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # Propagare inapoi (Backward pass). Calcularea gradientilor\n",
    "        grads = {}\n",
    "        #############################################################################\n",
    "        # TODO: Executarea unei propagari inapoi (backward pass). Calculam          # \n",
    "        # gradientul (derivata) loss-ului pentru fiecare parametru: weight si bias. #\n",
    "        # Stocam rezultatul in dictionarul grads. Grads['fc1_w'] ar trebui sa       #\n",
    "        # contina gradientul pentru parametrii fc1_w si sa fie o matrice de aceeasi #\n",
    "        # dimensiuni.                                                               #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=5e-6, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        In aceasta functie trebuie sa antrenam reteaua folosind stochastic gradient\n",
    "        descent.\n",
    "\n",
    "        Input:\n",
    "        - X: Un numpy array (train_dataset_size, input_dim) ce reprezinta input-ul de \n",
    "         train.\n",
    "        - y: Un numpy array (train_dataset_size,) ce reprezinta label-urile de train; \n",
    "         y[i] = c inseamna ca \n",
    "          imaginea X[i] are label-ul c, unde 0 <= c < num_classes \n",
    "        - X_val: Un numpy array (val_dataset_size, input_dim) ce reprezinta imaginile de \n",
    "         validare.\n",
    "        - y_val: Un numpy array (val_dataset_size,) ce reprezinta etichetele de validare.\n",
    "        - learning_rate: Un scalar ce reprezinta rata de invatare.\n",
    "        - learning_rate_decay: Un scalar ce reprezinta factorul folosit pentru decay-ul \n",
    "         ratei de invatare dupa fiecare epoca\n",
    "        - reg: Un scalar ce reprezinta factorul cu care se aplica regularizarea \n",
    "         parametrilor.\n",
    "        - num_iters: Numarul de iteratii folosit pentru optimizare.\n",
    "        - batch_size: Numarul de exemple folosite in fiecare mini-batch de optimizare.\n",
    "        - verbose: boolean; Flag ce seteaza print-uri in timpul antrenarii pentru a \n",
    "         afisa progresul.\n",
    "        \"\"\"\n",
    "        train_dataset_size = X.shape[0]\n",
    "        iterations_per_epoch = max(train_dataset_size / batch_size, 1)\n",
    "\n",
    "        # Folositi SGD pentru optimizare\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Samplati mini-batch-uri random din datele de antrenare (imagini #\n",
    "            # + labels) si le stocati in X_batch si y_batch                         #\n",
    "            #########################################################################\n",
    "            pass\n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            # Calculati costul(eroarea) si gradientii folosin mini-batch-ul curent\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Folositi gradientii stocati in dictionarul grads pentru a updata#\n",
    "            # parametrii retelei (stocati in dictionarul self.params) folosin SGD.   #\n",
    "            #########################################################################\n",
    "            pass\n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            # La fiecare epoca stocam acuratetea pe datale de antrenare si pe datale \n",
    "            # de validare si facem decay la rata de invatare\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Calculam acuratetea\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Facem decay la rata de invatare inmultind-o cu un numar subunitar\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "            'loss_history': loss_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        In aceasta functie trebuie sa folosim parametrii actuali ai retelei pentru a \n",
    "        prezice label-uri noi pentru noi exemple de imagini neetichetate.\n",
    "        Pentru fiecare exemplu X[i] o sa prezicem clasa cu probabilitate maxima din cele\n",
    "        num_classes.\n",
    "\n",
    "        Input:\n",
    "        - X: Un numpy array (test_dataset_size, image_dim) ce reprezinta imaginile ce \n",
    "        trebuie etichetate.\n",
    "\n",
    "        Iesire:\n",
    "        - y_pred: Un numpy array (test_dataset_size,) ce reprezinta etichetele prezise \n",
    "        pentru fiecare din elementele din X.\n",
    "        y_pred[i] = c inseamna ca pentru imaginiea X[i] am prezis ca aceasta are clasa c, \n",
    "        unde 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Aici trebuie sa implementam o prezicerea rezultatului pentru un   #\n",
    "        # un exemplu nou. Asta inseamna ca trebuie sa luam imaginea si sa o trecem#\n",
    "        # prin retea facand o propagare inainte. Dupa ce obtinem scorurile-logits #\n",
    "        # trebuie sa executam argmax de scoruri pentru a intoarce clasa cu scorul #\n",
    "        # cel mai mare. Nu trebuie sa mai facem probabilitatile finale pentru     #\n",
    "        # fiecare clasa, deoarece nu avem nevoie de ele.                          #\n",
    "        ###########################################################################\n",
    "        pass\n",
    "        ###########################################################################\n",
    "        #                              END OF YOUR CODE                           #\n",
    "        ###########################################################################\n",
    "\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gE1FLhMn_mvN"
   },
   "source": [
    "# Data de intrare de test\n",
    "Vom crea o retea simpla cu un singur strat ascuns si vom folosi niste date random pentru a testa corectitudinea implementarii unei propagari inainte si inapoi prin retea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YqEbxREpt3Hr"
   },
   "outputs": [],
   "source": [
    "from shallow_net import ShallowNet\n",
    "input_dim = 4\n",
    "hidden_dim = 10\n",
    "num_classes = 3\n",
    "dataset_size = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "net = ShallowNet(input_dim, hidden_dim, num_classes, std=1e-1)\n",
    "\n",
    "np.random.seed(1)\n",
    "X_train = 10 * np.random.randn(dataset_size, input_dim)\n",
    "y_train = np.array([0, 1, 2, 2, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yu3ho1zB7q3"
   },
   "source": [
    "# Propagare inainte (Forward pass)\n",
    "## Calcularea scorurilor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "heifQCr_CTCg"
   },
   "outputs": [],
   "source": [
    "scores = net.loss(X_train)\n",
    "print(scores)\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "# Differenta ar trebui sa fie foarte mica < 1e-7\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYpWbCtx5yYL"
   },
   "source": [
    "## Calcularea erorii\n",
    "In aceeasi functie trebuie sa definim partea a doua in care calculam eroare pentru setul de date de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "EuFgbfOB6J41"
   },
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X_train, y_train, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# diferenta ar trebui sa fie foarte mica < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kX87ZgHlrZgN"
   },
   "source": [
    "# Propagare inapoi (Backpropagation)\n",
    "Implementare completa a functiei loss pentru a cumprinde si gradientul functiei de cost pentru variabilele fc1_w, fc1_b, fc2_w, fc2_b.\n",
    "Pentru a testa corectitudienea implementarii gradientului putem folosi graientul numeric: (f(x-h) - f(x+h)) / 2h , h -> 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HZQJpRyGvH4L"
   },
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "from math import sqrt, ceil\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\" \n",
    "    Implementare naiva pentru calcularea gradientului numeric al functiei f\n",
    "    in punctul x \n",
    "    * f trebuie sa fie o functie care primeste un singur argument\n",
    "    * x este un punct intr-un spatiu n dimensional al parametrilor functiei f (numpy array) in care vrem sa evaluam gradientul\n",
    "    \"\"\" \n",
    "\n",
    "    fx = f(x) # valoare functiei in punctul initial\n",
    "    grad = np.zeros_like(x)\n",
    "    # iteram prin toate dimensiunile lui x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        # evaluam functia in x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # incrementam cu h\n",
    "        fxph = f(x) # evaluam f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluam f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # calculam derivata partial folosing formula centrata\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # panta\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # trecem la urmatoarea dimensiune\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def visualize_grid(Xs, ubound=255.0, padding=1):\n",
    "    \"\"\"\n",
    "    Redimensionarea unui tensor 4D reprezentand niste parametrii (filtre) pentru a fi vizualizat mai usor ca un grid.\n",
    "    Input:\n",
    "    - Xs: Parametrii de tip (N, H, W, C)\n",
    "    - ubound: Gridul de iesire va avea valori scalate intre [0, ubound]\n",
    "    - padding: Numarul de pixeli blank intre celulele \n",
    "    \"\"\"\n",
    "    (N, H, W, C) = Xs.shape\n",
    "    grid_size = int(ceil(sqrt(N)))\n",
    "    grid_height = H * grid_size + padding * (grid_size - 1)\n",
    "    grid_width = W * grid_size + padding * (grid_size - 1)\n",
    "    grid = np.zeros((grid_height, grid_width, C))\n",
    "    next_idx = 0\n",
    "    y0, y1 = 0, H\n",
    "    for y in range(grid_size):\n",
    "        x0, x1 = 0, W\n",
    "        for x in range(grid_size):\n",
    "            if next_idx < N:\n",
    "                img = Xs[next_idx]\n",
    "                low, high = np.min(img), np.max(img)\n",
    "                grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
    "                # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
    "                next_idx += 1\n",
    "            x0 += W + padding\n",
    "            x1 += W + padding\n",
    "        y0 += H + padding\n",
    "        y1 += H + padding\n",
    "    # grid_max = np.max(grid)\n",
    "    # grid_min = np.min(grid)\n",
    "    # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "goORMh6yrZgQ"
   },
   "outputs": [],
   "source": [
    "from utils import eval_numerical_gradient\n",
    "\n",
    "loss, grads = net.loss(X_train, y_train, reg=0.05)\n",
    "# diferenta ar trebui sa fie foarte mica < 1-8\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X_train, y_train, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "955D0vlQrZgX"
   },
   "source": [
    "# Antrenarea retelei (training)\n",
    "Vom antrena reteaua folosin stochastic gradient descent(SGD). \n",
    "In acest pas vom completa functia **train** din clasa ShallowNet. Vom completa de asemenea si functia **predict** pentru a afisa acuratetea retelei in timpul antrenarii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6ztsMUQxrZgZ"
   },
   "outputs": [],
   "source": [
    "stats = net.train(X_train, y_train, X_train, y_train, \n",
    "                  learning_rate=1e-1, reg=5e-6,\n",
    "                  num_iters=100, verbose=False)\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plotarea costului\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPt6-ZetZkyB"
   },
   "source": [
    "# BONUS\n",
    "# Incarcarea datasetului - CIFAR-10\n",
    "Vom incarca datasetul folosindu-ne de **keras**. \n",
    "\n",
    "**Keras** este un wrapper peste **tensorflow** **(framework de deep learning)**. In laboaratoarele viitoare veti folosi tensorflow pentru a scrie retele. Tensorflow are scrise deja biblioteci de functii in C++ ce folosesc rutine optimizate pentru a rula cod cuda pe GPU. Pe acestea le veti apela direct din python. \n",
    "\n",
    "In laboratorul de astazi incercam sa implementam totul de mana direct in python pentru a intelege \"magic\"-ul din spatele Tensorflow-ului."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_1xpZMja_e1"
   },
   "source": [
    "**CIFAR-10** este un dataset ce contine 50000 de imagini de train si 10000 imagini de test. Acestea eu dimensiuni 32x32x3. O sa spargem imaginile de train in doua splituri pentru a pastra 10000 de imagini pentru validare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "y7BwnpjfXwAW"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_val = X_train[40000:]\n",
    "X_train = X_train[:40000]\n",
    "y_val = y_train[40000:]\n",
    "y_train = y_train[:40000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ut9nJJBaafcX"
   },
   "source": [
    "Imaginile vor fi **vectorizate** inainte de a fi trimise retelei.\n",
    "\n",
    "**Numarul de clase din CIFAR-10** este (big surprise here!) 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTCHuWmCv4aZ"
   },
   "source": [
    "### Preprocesarea datasetului\n",
    "La acest pas, trebuie sa faceti o preprocesare minimala, normalizarea fiecarei imagini scazand valoarea medie peste tot datasetul. Imaginile sunt matrixi 32x32x3 cu tipul uint8, iar noi avem nevoie vectori de tip float32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LYeFH74Qa9FH"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    X_train = X_train.reshape(y_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(y_test.shape[0], -1)\n",
    "    X_val = X_val.reshape(y_val.shape[0], -1)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_val = X_val.astype('float32')\n",
    "    y_train = y_train.reshape(y_train.shape[0])\n",
    "    y_test = y_test.reshape(y_test.shape[0])\n",
    "    y_val = y_val.reshape(y_val.shape[0])\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_val -= mean_image\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Rmp6O9ZhY7Kj"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_dataset(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Val data shape: ', X_val.shape)\n",
    "print('Val labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6829NYnCw4Mc"
   },
   "source": [
    "## Antrenarea retelei pe CIFAR-10\n",
    "Pentru a antrena reteaua folosim SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_C1VVbfBxiYC"
   },
   "outputs": [],
   "source": [
    "input_size = 32\n",
    "input_channels = 3\n",
    "input_dim = input_channels * input_size * input_size\n",
    "hidden_dim = 50\n",
    "num_classes = 10\n",
    "net = ShallowNet(input_dim, hidden_dim, num_classes)\n",
    "# Antrenam reteaua\n",
    "stats = net.train(np.concatenate([X_train, X_test], 0), np.concatenate([y_train, y_test], 0), X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# Facem preziceri pe datasetul de test si calculam acuratetea\n",
    "test_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Test accuracy: ', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqZWQgqJrZhK"
   },
   "source": [
    "# Debugging pentru procesul de antrenare\n",
    "Test accuracy ar trebui sa fie cu parametrii de mai sus undeva la 0.28. Aceasta este o acuratete foarte mica, random ar fi undeva la 0.10, deci nu suntem foarte departe.\n",
    "Pentru a vedea ce sa intampla cu antrenarea putem plota valorile erorii pentru antrenare si test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "p_lx43_NrZhN"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Khbx-ke1rZhS"
   },
   "outputs": [],
   "source": [
    "from utils import visualize_grid\n",
    "\n",
    "# Vizualizarea parametrilor retelei\n",
    "\n",
    "def show_net_weights(net, param_key):\n",
    "    W1 = net.params[param_key]\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net, 'fc1_w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOSpkebYrZhV"
   },
   "source": [
    "# Tunarea hiperparametrilor\n",
    "Cateva observatii:\n",
    "* costul scade liniar ceea ce ar putea sugera ca putem mari rata de invatare\n",
    "* nu exista niciun gap intre plotul accuratetii pe datale de invatare vs datele de test ceea ce ar putea sugera ca suntem in regimul de 'underfitting' si avem un model cu capacitate prea mica.\n",
    "* pentru a dezvolta o intuitie de ce hiperparametrii merg in ce situatii, trebuie experimentat mult. Puteti experimenta cu diferite valori pentru urmatorii hiperparametrii: numarul de neuroni pe stratul ascuns (hidden_dim), rata de invatare etc. \n",
    "* in principiu ne dorim o acuratete pe datale de test > 48%.\n",
    "* pentru a ajunge la o acuratete foarte mare, sunt foarte multe trick-uri pe care le vom invata in episoadele urmatoare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sbjjivCArZhX"
   },
   "outputs": [],
   "source": [
    "best_net = net # cel mai bun model\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tunarea hiperparametrilor pe datele de validare.                        #\n",
    "# Cel mai bun model trebuie stocat in best_net.                                 #\n",
    "# Hint: Cea mai simpla tunare poate fi o iterare prin mai multe valori ale      #                            #\n",
    "# parametrilor pentru hidden_dim, learning_rate, l2_reg, etc.  Departajarea     #\n",
    "# celui mai bun model se poate face dupa acuratete                              #\n",
    "#################################################################################\n",
    "pass\n",
    "#################################################################################\n",
    "#                               END OF YOUR CODE                                #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Vx50Kj_YrZhf"
   },
   "outputs": [],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net, 'fc1_w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqzwoAYorZhn"
   },
   "source": [
    "# Rularea pe datele de test \n",
    "Testul final al unei retele este rularea acesteia pe datele de test. De obiecei acestea sunt niste date pastrate deoparte pentru care nu avem label-urile/etichetele. De exemplu, in cadrul unei competitii, se vor publica datele de antrenare cu label-urile corespunzatoare acestora pentru antrenarea unui model, datele de validare cu label-urile corespunzatoare acestora pentru tunarea hiperparametrilor dupa antrenare, si datele de testare fara label-uri. Submisia in cardul unei competitii este facuta cu prezicerile modelului pentru datele de test.\n",
    "\n",
    "In cazul de fata, noi vom fi si evaluatori, avand deja etichetele reale, putem sa ne calculam singuri acuratetea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ujnQxx0SrZhv"
   },
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "-3-qIO9orZh2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "solver.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
